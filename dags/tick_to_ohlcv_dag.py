# dags/tick_to_ohlcv.py
from __future__ import annotations

import os
from datetime import datetime
from pathlib import Path

import pandas as pd
import polars as pl
import sqlalchemy as sa
from airflow.decorators import dag, task
from dotenv import load_dotenv

load_dotenv()

KOSPI_RAW_DIR = Path("/opt/airflow/data/KOSPI/unzip")
KOSPI_OUT_DIR = Path("/opt/airflow/data/KOSPI/ohlcv")
KOSDAQ_RAW_DIR = Path("/opt/airflow/data/KOSDAQ/unzip")
KOSDAQ_OUT_DIR = Path("/opt/airflow/data/KOSDAQ/ohlcv")
TZ = "Asia/Seoul"

COLS = [
    "ì¼ì",
    "ëŒ€ëŸ‰ë§¤ë§¤êµ¬ë¶„ì½”ë“œ",
    "ì •ê·œì‹œê°„ì™¸êµ¬ë¶„ì½”ë“œ",
    "ì¢…ëª©ì½”ë“œ",
    "ì¢…ëª©ì¸ë±ìŠ¤",
    "ì²´ê²°ë²ˆí˜¸",
    "ì²´ê²°ê°€ê²©",
    "ì²´ê²°ìˆ˜ëŸ‰",
    "ì²´ê²°ìœ í˜•ì½”ë“œ",
    "ì²´ê²°ì¼ì",
    "ì²´ê²°ì‹œê°",
    "ê·¼ì›”ë¬¼ì²´ê²°ê°€ê²©",
    "ì›ì›”ë¬¼ì²´ê²°ê°€ê²©",
    "ë§¤ìˆ˜íšŒì›ë²ˆí˜¸",
    "ë§¤ìˆ˜í˜¸ê°€ìœ í˜•ì½”ë“œ",
    "ë§¤ìˆ˜ìì‚¬ì£¼ì‹ ê³ ì„œID",
    "ë§¤ìˆ˜ìì‚¬ì£¼ë§¤ë§¤ë°©ë²•ì½”ë“œ",
    "ë§¤ìˆ˜ë§¤ë„ìœ í˜•ì½”ë“œ",
    "ë§¤ìˆ˜ìœ„íƒìê¸°êµ¬ë¶„ì½”ë“œ",
    "ë§¤ìˆ˜ìœ„íƒì‚¬ë²ˆí˜¸",
    "ë§¤ìˆ˜PTêµ¬ë¶„ì½”ë“œ",
    "ë§¤ìˆ˜íˆ¬ììêµ¬ë¶„ì½”ë“œ",
    "ë§¤ìˆ˜ì™¸êµ­ì¸íˆ¬ììêµ¬ë¶„ì½”ë“œ",
    "ë§¤ìˆ˜í˜¸ê°€ì ‘ìˆ˜ë²ˆí˜¸",
    "ë§¤ë„íšŒì›ë²ˆí˜¸",
    "ë§¤ë„í˜¸ê°€ìœ í˜•ì½”ë“œ",
    "ë§¤ë„ìì‚¬ì£¼ì‹ ê³ ì„œID",
    "ë§¤ë„ìì‚¬ì£¼ë§¤ë§¤ë°©ë²•ì½”ë“œ",
    "ë§¤ë„ë§¤ë„ìœ í˜•ì½”ë“œ",
    "ë§¤ë„ìœ„íƒìê¸°êµ¬ë¶„ì½”ë“œ",
    "ë§¤ë„ìœ„íƒì‚¬ë²ˆí˜¸",
    "ë§¤ë„PTêµ¬ë¶„ì½”ë“œ",
    "ë§¤ë„íˆ¬ììêµ¬ë¶„ì½”ë“œ",
    "ë§¤ë„ì™¸êµ­ì¸íˆ¬ììêµ¬ë¶„ì½”ë“œ",
    "ë§¤ë„í˜¸ê°€ì ‘ìˆ˜ë²ˆí˜¸",
    "ì‹œê°€",
    "ê³ ê°€",
    "ì €ê°€",
    "ì§ì „ê°€ê²©",
    "ëˆ„ì ì²´ê²°ìˆ˜ëŸ‰",
    "ëˆ„ì ê±°ë˜ëŒ€ê¸ˆ",
    "ìµœì¢…ë§¤ë„ë§¤ìˆ˜êµ¬ë¶„ì½”ë“œ",
    "LPë³´ìœ ìˆ˜ëŸ‰",
    "ë°ì´í„°êµ¬ë¶„",
    "ë©”ì„¸ì§€ì¼ë ¨ë²ˆí˜¸",
    "ë§¤ìˆ˜í”„ë¡œê·¸ë¨í˜¸ê°€ì‹ ê³ êµ¬ë¶„ì½”ë“œ",
    "ë§¤ë„í”„ë¡œê·¸ë¨í˜¸ê°€ì‹ ê³ êµ¬ë¶„ì½”ë“œ",
    "ë³´ë“œID",
    "ì„¸ì…˜ID",
    "ì‹¤ì‹œê°„ìƒí•œê°€",
    "ì‹¤ì‹œê°„í•˜í•œê°€",
]
KEEP = ["ì¢…ëª©ì½”ë“œ", "ì²´ê²°ê°€ê²©", "ì²´ê²°ìˆ˜ëŸ‰", "ì²´ê²°ì¼ì", "ì²´ê²°ì‹œê°"]


@dag(
    start_date=datetime(2025, 7, 4),
    schedule="@once",
    catchup=False,
    tags=["ticks", "ohlcv"],
    max_active_tasks=1,
)
def tick_to_ohlcv_dag():

    @task
    def get_files_to_process(year_month_list: list[str]) -> list[str]:
        """Return all raw tick files that *don't* have a matching parquet yet."""
        kospi_files = [
            str(p)
            for p in KOSPI_RAW_DIR.iterdir()
            if p.name.endswith(".dat")
            and any(year_month in p.name for year_month in year_month_list)
        ]

        kosdaq_files = [
            str(p)
            for p in KOSDAQ_RAW_DIR.iterdir()
            if p.name.endswith(".dat")
            and any(year_month in p.name for year_month in year_month_list)
        ]

        return kospi_files + kosdaq_files

    @task
    def convert(file_path: str) -> list[str]:
        """
        (1) tick íŒŒì¼ì— í¬í•¨ëœ ë‚ ì§œ ëª©ë¡ì„ ë½‘ê³ ,
        (2) disclosure_eventsì—ì„œ ê°™ì€ ë‚ ì§œì— ê³µì‹œê°€ ë‚˜ì˜¨ ì¢…ëª©ì½”ë“œ universeë¥¼ ë§Œë“¤ê³ ,
        (3) universe ì•ˆì— ìˆëŠ” ì¢…ëª©ë§Œ ê·¸ë£¹í•‘í•˜ì—¬ 1-ë¶„ OHLCVë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
        """
        p = Path(file_path)
        out_paths: list[str] = []

        # íŒŒì¼ ê²½ë¡œì— ë”°ë¼ ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²°ì •
        if "KOSPI" in str(p):
            OUT_DIR = KOSPI_OUT_DIR
        elif "KOSDAQ" in str(p):
            OUT_DIR = KOSDAQ_OUT_DIR
        else:
            raise ValueError(f"Unknown market type for file: {p}")

        # â”€â”€ â‘  tick íŒŒì¼ ì•ˆì— ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” 'ì²´ê²°ì¼ì' ëª©ë¡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        dates: list[int] = (
            pl.scan_csv(
                p,
                separator="|",
                has_header=False,
                new_columns=COLS,
                low_memory=True,
            )
            .select(["ì²´ê²°ì¼ì"])
            .unique()
            .sort("ì²´ê²°ì¼ì")
            .collect(streaming=True)["ì²´ê²°ì¼ì"]
            .to_list()
        )

        print(dates)
        if not dates:
            return out_paths  # ë¹„ì–´ ìˆìœ¼ë©´ ë°”ë¡œ ì¢…ë£Œ

        # â”€â”€ â‘¡ disclosure_eventsì—ì„œ ê³µì‹œê°€ ìˆì—ˆë˜ ì¢…ëª© universe í™•ë³´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        #     ê³µì‹œ í…Œì´ë¸”ì€ YYYYMMDD â†’ ì¢…ëª©ì½”ë“œ ë°°ì—´(dict) í˜•íƒœë¡œ ë³€í™˜

        POSTGRES_USER = os.getenv("POSTGRES_USER")
        POSTGRES_PASSWORD = os.getenv("POSTGRES_PASSWORD")
        POSTGRES_DB = os.getenv("POSTGRES_DB")
        POSTGRES_PORT = os.getenv("POSTGRES_PORT", "5432")

        db_url = f"postgresql+psycopg2://{POSTGRES_USER}:{POSTGRES_PASSWORD}@postgres_events:{POSTGRES_PORT}/{POSTGRES_DB}"
        engine = sa.create_engine(db_url, pool_pre_ping=True, future=True)
        # 1ï¸âƒ£ íŒŒì´ì¬ì—ì„œ date ë¡œ ë³€í™˜
        dates_date = [datetime.strptime(str(d), "%Y%m%d").date() for d in dates]

        with engine.connect() as conn:
            sql = sa.text(
                """
                SELECT
                    -- UTC â†’ KST ë³€í™˜ í›„ ë‚ ì§œë§Œ ì¶”ì¶œ
                    (disclosed_at AT TIME ZONE 'Asia/Seoul')::date AS disclosed_at,
                    stock_code
                FROM disclosure_events
                WHERE (disclosed_at AT TIME ZONE 'Asia/Seoul')::date IN :dates
                    AND stock_code IS NOT NULL
                """
            ).bindparams(sa.bindparam("dates", expanding=True))

            # DB â†’ pandas
            df_events = pd.read_sql(sql, conn, params={"dates": dates_date})

        # pandas â†’ Polars â†’ ì§‘ê³„
        events_pl = pl.from_pandas(df_events)
        events = events_pl.group_by("disclosed_at").agg(
            pl.col("stock_code").unique().alias("codes")
        )

        # {YYYYMMDD: [code, â€¦]} ë”•ì…”ë„ˆë¦¬
        universe: dict[int, list[str]] = {
            int(d.strftime("%Y%m%d")): codes
            for d, codes in zip(
                events["disclosed_at"].to_list(), events["codes"].to_list()
            )
        }

        # â”€â”€ â‘¢ ë‚ ì§œë³„ ë³€í™˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        for d in dates:
            print("date: ", d)
            codes = universe.get(d, [])
            if not codes:  # ê³µì‹œ ì¢…ëª©ì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ
                continue

            date_str = f"{d:08d}"
            out = OUT_DIR / f"{date_str}_1m.parquet"
            if out.exists():
                continue

            (
                pl.scan_csv(
                    p,
                    separator="|",
                    has_header=False,
                    new_columns=COLS,
                    low_memory=True,
                    infer_schema_length=1000,
                )
                .select(KEEP)
                .filter(
                    (pl.col("ì²´ê²°ì¼ì") == d)
                    & (pl.col("ì¢…ëª©ì½”ë“œ").is_in(codes))  # â† **ê³µì‹œ ì¢…ëª© í•„í„°**
                )
                .with_columns(
                    pl.datetime(
                        (pl.col("ì²´ê²°ì¼ì") // 10_000),
                        ((pl.col("ì²´ê²°ì¼ì") // 100) % 100).cast(pl.UInt8),
                        (pl.col("ì²´ê²°ì¼ì") % 100).cast(pl.UInt8),
                        (pl.col("ì²´ê²°ì‹œê°") // 10_000_000).cast(pl.UInt8),
                        ((pl.col("ì²´ê²°ì‹œê°") // 100_000) % 100).cast(pl.UInt8),
                        ((pl.col("ì²´ê²°ì‹œê°") // 1_000) % 100).cast(pl.UInt8),
                        (pl.col("ì²´ê²°ì‹œê°") % 1_000) * 1_000,
                        time_zone=TZ,
                    ).alias("ts")
                )
                .drop(["ì²´ê²°ì¼ì", "ì²´ê²°ì‹œê°"])
                .sort("ts")
                .group_by_dynamic(
                    index_column="ts", every="1m", by="ì¢…ëª©ì½”ë“œ", closed="left"
                )
                .agg(
                    open=pl.col("ì²´ê²°ê°€ê²©").first(),
                    high=pl.col("ì²´ê²°ê°€ê²©").max(),
                    low=pl.col("ì²´ê²°ê°€ê²©").min(),
                    close=pl.col("ì²´ê²°ê°€ê²©").last(),
                    volume=pl.col("ì²´ê²°ìˆ˜ëŸ‰").sum(),
                )
                .sink_parquet(out, compression="zstd", row_group_size=50_000)
            )

            out_paths.append(str(out))

        return out_paths

    # ğŸ‰ Dynamic mapping fan-out
    files = get_files_to_process(year_month_list=["2023_12"])
    convert.expand(file_path=files)


dag = tick_to_ohlcv_dag()
